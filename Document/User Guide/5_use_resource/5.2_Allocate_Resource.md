[userguide]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide#-%EB%AA%A9%EC%B0%A8
[ohpc]: http://openhpc.community/
[slurm]: https://slurm.schedmd.com/

[4]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide/4_app_env
[4.1]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.1_Anaconda.md
[4.2]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.2_Module.md
[4.3]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.3_Docker.md
[4.4]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.4_Singularity.md

[5]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide/5_use_resource
[5.1]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.1_Resource_manager_Intro.md
[5.2]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md
[5.3]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.3_Priority_submitted_job_and_start_time.md

## [## 5.2  클러스터 자원 배정][5]  

**자원 관리자(Resource Manager)** 를 통해 리소스(node 및 GPU)를 배정받고  
[4. 응용프로그램 사용환경 구성 및 시험][4] 에서 시험해 보았던 샘플 코드를  
배정받은 리소스(node 및 GPU) 에서 실행해 보겠습니다.

### [### 5.2.1  srun 을 이용한  Interactive (대화형) 작업][5.2]
```bash
# gpu 1개를 사용할 수 있는 리소스(node) 요청, 사용 시간은 1시간으로 지정.
srun  --gres=gpu:1  --time=1:00:00  --pty bash -i

# 할당된 작업의 대기열 (queue)상태 확인.
squeue -u $USER
squeuelong  -u $USER

# 할당 받은 gpu 번호 확인.
echo $CUDA_VISIBLE_DEVICES

# 노드의 gpu 상태 확인.
nvidia-smi
```

#### #### Anaconda 의 경우 #1
기존 방법대로 실행할 수 있습니다. (RTX 계열 그래픽카드의 경우   
cuda 11버젼이 필요하므로  #2 방법으로 진행해야 합니다.)

[### 4.1.3 Anaconda 환경에서 TensorFlow Sample Code 실행.][4.1]

```bash
# 생성된 환경 목록 확인.
conda env list

# 환경 활성화.
conda activate   NEW-py36-tf1.11-cuda9.0  

# Sample Code 를 실행해 보기 전에 gpu 상태 확인
gpustat

nvidia-smi

# Sample Code 실행
python  ~/TensorFlow-Examples/examples/3_NeuralNetworks/neural_network_raw.py
```

#### #### Anaconda 의 경우 #2 (RTX 계열 GPU)
cuda 를 제외한 conda 환경을 생성한 후 cuda는 module 을 통해 11.2 를 load 해서 사용 합니다.
```bash
conda deactivate
conda create   -n py36-tf1.14    -c anaconda    python=3.6

conda activate   py36-tf1.14
pip install     tensorflow-gpu==1.14  matplotlib

# Sample Code 를 실행해 보기 전에 gpu 상태 확인
gpustat
nvidia-smi

# Sample Code 실행 1
python  ~/TensorFlow-Examples/examples/3_NeuralNetworks/neural_network_raw.py

# Sample Code 실행 2
python  ~/TensorFlow-Examples/examples/3_NeuralNetworks/dcgan.py
```

***

#### #### Docker 의 경우  
기존 방법에 `-e NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES ` 옵션을 추가해서 실행 해야 합니다.  
SLURM을 통해 할당받은 GPU 번호를 Docker 의 컨테이너에게 전달하는 부분이며   
**이 옵션을 누락할 경우 다른 사용자의 작업이 돌고 있는 GPU 에 중첩**해서 작업이 돌아갈 수 있어서   
기존 작업과 새로운 **작업이 모두 중단**될 가능성이 높습니다.  

[### 4.3.4 TensorFlow Docker 컨테이너로 파이썬 코드 실행. (--runtime=nvidia 옵션)][4.3]

아래와 같이 실행할 수 있습니다.

```bash
### sample data file 이 download 되는 tmp 폴더도 연결 시킵니다.
mkdir /tmp/$USER

docker run -u $UID:$GROUPS --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES \
   --rm  -v ~:/home/$USER  -v /tmp/$USER:/tmp  \
   tensorflow/tensorflow:1.11.0-gpu-py3  python  ~/TensorFlow-Examples/examples/3_NeuralNetworks/neural_network_raw.py

```

#### #### singularity 의 경우
기존 방법대로 실행할 수 있습니다.

```bash
### sample data file 이 download 되는 tmp 폴더도 연결 시킵니다.
mkdir /tmp/$USER

singularity exec --nv ~/tf-1.11-gpu-py3.simg  -B /tmp/$USER:/tmp \
   python  ~/TensorFlow-Examples/examples/3_NeuralNetworks/neural_network_raw.py

```



***

## [끝][5.2]
