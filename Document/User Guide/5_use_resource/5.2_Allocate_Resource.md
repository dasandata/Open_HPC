[userguide]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide#-%EB%AA%A9%EC%B0%A8
[ohpc]: http://openhpc.community/
[slurm]: https://slurm.schedmd.com/

[4]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide/4_app_env
[4.1]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.1_Anaconda.md
[4.2]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.2_Module.md
[4.3]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.3_Docker.md
[4.4]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/4_app_env/4.4_Singularity.md

[5]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide/5_use_resource
[5.1]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.1_Resource_manager_Intro.md
[5.2]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md
[5.3]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.3_Priority_submitted_job_and_start_time.md

[5.2.1]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-521--srun-%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C--interactive-%EB%8C%80%ED%99%94%ED%98%95-%EC%9E%91%EC%97%85-1
[5.2.1-anaconda]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-srun---anaconda-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1
[5.2.1-module]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-srun---module-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1
[5.2.1-docker]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-srun---docker-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1
[5.2.1-singularity]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-srun---singularity-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1

[5.2.2]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-522--sbatch-%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C--batch-%EC%9D%BC%EA%B4%84%ED%98%95-%EC%9E%91%EC%97%85-1
[5.2.2-anaconda]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-sbatch---anaconda-%EC%9E%91%EC%97%85-%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8-%EC%9E%91%EC%84%B1%EC%9D%98-%EC%98%88
[5.2.2-module]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-sbatch---module-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1
[5.2.2-docker]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-sbatch---docker-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1
[5.2.2-singularity]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md#-sbatch---singularity-%EC%9D%98-%EA%B2%BD%EC%9A%B0-1


## [## 5.2  클러스터 자원 배정][5]  

**자원 관리자(Resource Manager)** 를 통해 리소스(node 및 GPU)를 배정받고  
[4. 응용프로그램 사용환경 구성 및 시험][4] 에서 시험해 보았던 샘플 코드를  
배정받은 리소스(node 및 GPU) 에서 실행해 보겠습니다.

## ## 목차

### [### 5.2.1  srun 을 이용한  Interactive (대화형) 작업][5.2.1]
#### [#### srun - Anaconda 의 경우][5.2.1-anaconda]
#### [#### srun - Module 의 경우][5.2.1-module]
#### [#### srun - Docker 의 경우][5.2.1-docker]
#### [#### srun - singularity 의 경우][5.2.1-singularity]

### [### 5.2.2  sbatch 를 이용한  Batch (일괄형) 작업][5.2.2]
#### [#### sbatch - Anaconda 의 경우][5.2.2-anaconda]
#### [#### sbatch - Module 의 경우][5.2.2-module]
#### [#### sbatch - Docker 의 경우][5.2.2-docker]
#### [#### sbatch - singularity 의 경우][5.2.2-singularity]

***

### [### 5.2.1  srun 을 이용한  Interactive (대화형) 작업][5.2]

srun 을 이용한 Interactive (대화형) 작업은 실시간으로 명령을 입력하과 결과를 확인할 수 있어  
의도했던 작업이 잘 수행 되는지 즉시 확실할 수 있는 장점이 있지만,    
작업이 끝나도 자동으로 종료되지 않으므로 **클러스터 자원이 낭비**되는 요인이 될 수 있습니다.   
<br>
작업 시작시 제한시간을 지정 하거나 가능하다면 디버깅 용도로만 사용는 것을 권장 하며,   
일부 클러스터 시스템 에서는 srun 을 통한 작업의 제한시간이 짧게 적용 되는 경우가 있습니다.    

```bash
cd ~

# 전체 노드의 상태
sinfo
sinfolong

# 클러스터의 대기열 (queue)상태 확인.
echo  $USER   ## 현재 사용자의 id
squeue      -u $USER
squeuelong  -u $USER

# gpu 1개를 사용할 수 있는 리소스(node) 요청, 사용 시간은 1시간으로 지정.
srun  --gres=gpu:1  --time=1:00:00  --pty bash -i

# 리소스를 할당 받은 후 할당된 작업의 대기열 (queue)상태 재 확인.
echo  $USER   ## 현재 사용자의 id
squeue      -u $USER
squeuelong  -u $USER

# 할당 받은 gpu 번호 확인.
echo  $CUDA_VISIBLE_DEVICES

# 노드의 gpu 상태 확인.
gpustat

nvidia-smi
```

#### #### srun - Anaconda 의 경우
**RTX 계열 GPU 의 경우 cuda 11.0** 버젼이 필요 합니다.
conda로 python 3.8.5 환경을 생성하고 cuda는 module 을 통해 load 하여 사용 합니다.

```bash
cd ~

# conda 환경 생성.
conda deactivate
conda create   -n py38-tf2.4   python==3.8.5  
conda activate    py38-tf2.4

pip install      tensorflow-gpu==2.4  matplotlib

ml purge
ml load cuda/11.0

# Tensorflow 2.4용 샘플파일 다운로드.
wget https://raw.githubusercontent.com/dasandata/Open_HPC/master/Document/User%20Guide/6_etc/dcgan-py38-tf2.4-cuda11.0.py

# Sample Code 를 실행해 보기 전에 gpu 상태 확인
gpustat

nvidia-smi

# Sample Code 실행 1
python  ~/dcgan-py38-tf2.4-cuda11.0.py
```

***

#### #### srun - Module 의 경우
리소스를 배정 받아 계산 노드로 진입한 후에도 login 노드와 동일하게 module 명령을 사용할 수 있습니다.

```bash
ml

ml purge
ml

which nvcc ; echo ; nvcc -V

ml av | grep cuda
ml load cuda/10.0

which nvcc ; echo ; nvcc -V

ml swap   cuda/10.0  cuda/11.0
which nvcc ; echo ; nvcc -V
```

***

#### #### srun - Docker 의 경우  
기존 방법에 `-e NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES ` 옵션을 추가해서 실행 해야 합니다.  
SLURM을 통해 할당받은 GPU 번호 - $CUDA_VISIBLE_DEVICES) - 를 Docker 의 컨테이너에게 전달하는 부분이며   
**이 옵션을 누락할 경우 다른 사용자의 작업이 돌고 있는 GPU 에 중첩**해서 작업이 돌아갈 수 있어서   
기존 작업과 새로운 **작업이 모두 중단**될 가능성이 높습니다.  

***

**RTX 계열 GPU 의 경우 cuda 11.0** 버젼이 필요 하므로  
**cuda 11.0 과 tessorflow 2.4** 로 구성된 새로운 Docker image를 내려 받아 진행 합니다.  

```bash
cd ~

# Tensorflow 2.4용 샘플파일 다운로드.
wget https://raw.githubusercontent.com/dasandata/Open_HPC/master/Document/User%20Guide/6_etc/dcgan-py38-tf2.4-cuda11.0.py

# Docker Imaage 다운로드(pull)
docker pull   dasandata/tensorflow:2.4.1-gpu-py3.8-cuda11.0

# 실행.
docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES \
   --rm  -v ~:/home/$USER  \
    dasandata/tensorflow:2.4.1-gpu-py3.8-cuda11.0 \
    python  ~/dcgan-py38-tf2.4-cuda11.0.py

```

***

#### #### srun - singularity 의 경우 #1
앞서 실행한 Docker image를 singularity image 로 build 한 후 사용 합니다.

```bash
cd ~

# Tensorflow 2.4용 샘플파일 다운로드.
wget https://raw.githubusercontent.com/dasandata/Open_HPC/master/Document/User%20Guide/6_etc/dcgan-py38-tf2.4-cuda11.0.py

# singularity Module Load
ml purge
ml load singularity

singularity  build     ~/tf-2.4-gpu-py3.8-cuda11.0.simg   docker://dasandata/tensorflow:2.4.1-gpu-py3.8-cuda11.0

singularity exec --nv  ~/tf-2.4-gpu-py3.8-cuda11.0.simg   python  ~/dcgan-py38-tf2.4-cuda11.0.py

```

#### #### srun - singularity 의 경우 #2 (module을 통해 cuda 사용)

singularity(docker)이미지 내에 cuda가 포함되어 있지 않은 경우    
module 과 singularity 의 bind 옵션을 사용해서 cuda 를 사용할 수 있습니다.   

```bash
cd ~

# singularity Module Load
ml purge
ml load singularity

# 이미지 생성 확인.
singularity  build     ~/tf-2.4-gpu-py3.8-nocuda.simg   docker://dasandata/tensorflow:2.4.1-gpu-py3.8-nocuda

# images 내부에 선언된 $LD_LIBRARY_PATH 확인.
singularity exec --nv  ~/tf-2.4-gpu-py3.8-nocuda.simg     echo $LD_LIBRARY_PATH

# cuda 11.0 Module Load
ml load cuda/11.0

# images 내부에 선언된 $LD_LIBRARY_PATH 변화 확인.
singularity exec --nv  ~/tf-2.4-gpu-py3.8-nocuda.simg     echo $LD_LIBRARY_PATH
singularity exec --nv  ~/tf-2.4-gpu-py3.8-nocuda.simg     ls -l /opt/

### module 을 통해 load 된 cuda를 사용할 수 있도록 /opt/ohpc 를 이미지와 연결(Bind)
singularity exec --nv  -B /opt/ohpc:/opt/ohpc   ~/tf-2.4-gpu-py3.8-nocuda.simg \
    python   ~/dcgan-py38-tf2.4-cuda11.0.py
```

***

### [### 5.2.2  sbatch 를 이용한  Batch (일괄형) 작업][5.2]

앞서 srun 으로 Interactive (대화형) 작업들을 스크립트(Script) 파일로 만들어서   
sbatch 명령으로 노드에 작업을 제출(submit) 해 보겠습니다.  
<br>

스크립트를 통한 작업 제출의 예)
```bash
cd ~

# 전체 노드의 상태
sinfo
sinfolong

# 클러스터의 대기열 (queue)상태 확인.
echo  $USER   ## 현재 사용자의 id
squeue      -u $USER
squeuelong  -u $USER

## 스크립트 작성
cat << EOF > ~/1min-job.sh
#!/bin/bash

echo "### 1 min job start"
echo "###"
echo "### START DATE=\$(date)"
echo "### HOSTNAME=\$(hostname)"
echo "### CUDA_VISIBLE_DEVICES=\$CUDA_VISIBLE_DEVICES"
echo "###"
echo "### sleep 1min..."
sleep \$((60*1))
echo "###"
echo "### END DATE=\$(date)"

EOF

# 작성된 작업 파일 내용 확인.
cat ~/1min-job.sh

# gpu 1개를 사용할 수 있는 리소스(node)에, 사용 시간은 10분으로 지정하여 작업을 제출(submit)
## --time=일-시간:분:초
sbatch  --gres=gpu:1  --time=10:00   ~/1min-job.sh

# 작업이 제출 된 후 대기열 (queue) 과 노드(node) 상태 재 확인.
squeue      -u $USER
squeuelong  -u $USER

sinfo
sinfolong
```

#### #### sbatch - Anaconda 작업 스크립트 작성의 예.

```bash
cd ~

# 작업 스크립트 작성.
cat << EOF > ~/anaconda-job.sh
#!/bin/bash

echo "### START DATE=\$(date)"
echo "### HOSTNAME=\$(hostname)"
echo "### CUDA_VISIBLE_DEVICES=\$CUDA_VISIBLE_DEVICES"

# conda 환경 활성화.
source  ~/.bashrc
conda   activate   py38-tf2.4

# cuda 11.0 환경 구성.
ml purge
ml load cuda/11.0

# 활성화된 환경에서 코드 실행.
python  ~/dcgan-py38-tf2.4-cuda11.0.py

echo "###"
echo "### END DATE=\$(date)"
EOF

# 작성된 작업 스크립트 확인.
cat ~/anaconda-job.sh

# gpu 1개를 사용할 수 있는 리소스(node)에, 사용 시간은 10분으로 지정하여 작업을 제출(submit)
## --time=일-시간:분:초
sbatch  --gres=gpu:1  --time=10:00  ~/anaconda-job.sh
```

***

#### #### sbatch - Module 의 경우

[sbatch - Anaconda 작업 스크립트 작성의 예] 처럼.  
스크립트 내에서 module 명령을 사용하여 환경을 구성할 수 있습니다.  

***

#### #### sbatch - Docker 의 경우  

```bash
cd ~

# 작업 스크립트 작성.
cat << EOF > ~/docker-job.sh
#!/bin/bash

echo "### START DATE=\$(date)"
echo "### HOSTNAME=\$(hostname)"
echo "### CUDA_VISIBLE_DEVICES=\$CUDA_VISIBLE_DEVICES"

docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=\$CUDA_VISIBLE_DEVICES \
   --rm  -v ~:/home/\$USER  \
    dasandata/tensorflow:2.4.1-gpu-py3.8-cuda11.0 \
    python  ~/dcgan-py38-tf2.4-cuda11.0.py

echo "###"
echo "### END DATE=\$(date)"
EOF

# 작성된 작업 스크립트 확인.
cat ~/docker-job.sh

# gpu 1개를 사용할 수 있는 리소스(node)에, 사용 시간은 10분으로 지정하여 작업을 제출(submit)
## --time=일-시간:분:초
sbatch  --gres=gpu:1  --time=10:00  ~/docker-job.sh
```

***

#### #### sbatch - singularity 의 경우 #1

```bash
cd ~

# 작업 스크립트 작성.
cat << EOF > ~/singularity-job.sh
#!/bin/bash

echo "### START DATE=\$(date)"
echo "### HOSTNAME=\$(hostname)"
echo "### CUDA_VISIBLE_DEVICES=\$CUDA_VISIBLE_DEVICES"

# singularity Module Load
ml purge
ml load singularity

singularity exec --nv  ~/tf-2.4-gpu-py3.8-cuda11.0.simg   python  ~/dcgan-py38-tf2.4-cuda11.0.py

echo "###"
echo "### END DATE=\$(date)"
EOF

# 작성된 작업 스크립트 확인.
cat ~/singularity-job.sh

# gpu 1개를 사용할 수 있는 리소스(node)에, 사용 시간은 10분으로 지정하여 작업을 제출(submit)
## --time=일-시간:분:초
sbatch  --gres=gpu:1  --time=10:00  ~/singularity-job.sh
```

***

## [끝][5.2]
