[userguide]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide#-%EB%AA%A9%EC%B0%A8
[ohpc]: http://openhpc.community/
[slurm]: https://slurm.schedmd.com/
[5]: https://github.com/dasandata/Open_HPC/tree/master/Document/User%20Guide/5_use_resource
[5.1]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.1_Resource_manager_Intro.md
[5.2]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.2_Allocate_Resource.md
[5.3]: https://github.com/dasandata/Open_HPC/blob/master/Document/User%20Guide/5_use_resource/5.3_Priority_submitted_job_and_start_time.md

## [## 5.2  클러스터 자원배정(요청)][5]  

**자원관리자(Resource Manager)** 는 보통 HPC 클러스터 환경에서만 접할 수 있는 도구로,
기본적으로 아래 그림과 같이(Backfil) 작업의 크기에 따라 가용한 자원에 작업을 분배하고  
자원이 모두 사용중일 때는 **대기열(queue)** 에 작업을 쌓아 두었다가  
사용가능한 자원이 확보되면, 대기하고 있던 작업을 시작시켜주는 역할을 하게 됩니다.

### [### 5.2.1  dfdfasdfsafd ][5.2]






```
--runtime=nvidia  -e NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES


docker run  --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES   --rm  -v ~:/home/$USER  tensorflow/tensorflow:1.11.0-gpu-py3   python   ~/TensorFlow-Examples/examples/3_NeuralNetworks/neural_network.py

## docker sbatch
```


```bash
# UID, GID 확인.
id  


#
docker run -u $UID:$GROUPS --shm-size=4g --ulimit memlock=-1 --ulimit stack=67108864  --gpus "device=$CUDA_VISIBLE_DEVICES" --rm -ti -v /home/sonic/TensorFlow-2.x-Tutorials/03-Play-with-MNIST/:/mnt  tensorflow/tensorflow:latest-gpu  python /mnt/main.py
```





***

## [끝][5.2]
